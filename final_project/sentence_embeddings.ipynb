{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labels import classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymac/miniforge3/envs/501r/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'sentence-transformers/paraphrase-distilroberta-base-v1'\n",
    "model = SentenceTransformer(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_text = []\n",
    "for vals in classes.values():\n",
    "    classes_text += [key for key in vals['subclasses'].keys()]\n",
    "classes_text = np.array(classes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_embeddings = model.encode(classes_text, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 (124, 768)\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_text), classes_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence_embedding: np.array, class_embeddings: np.array, top_n=5) -> np.array:\n",
    "    similarities = np.array(util.cos_sim(sentence_embedding, class_embeddings)).reshape(-1,)\n",
    "    top_n_indices = np.argsort(similarities)[::-1][0:top_n]\n",
    "\n",
    "    return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{player_names}, would you like to join our group?',\n",
       "       'Add {player_names} to the group',\n",
       "       'Add {player_names} to the group?', 'To make more friends',\n",
       "       '{player_names} want to join'], dtype='<U64')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Asiago and Blue are looking for friends.  We could add them to our group for a mega-group'\n",
    "text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "indices = convert(text_embedding, classes_embeddings)\n",
    "classes_text[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>submessage_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ill send 3 if you send 3 back?</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perhaps</td>\n",
       "      <td>7</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perhaps not</td>\n",
       "      <td>7</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sending 4 back to you this round</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u and me dawg lets win this thing</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  label  submessage_label\n",
       "0     Ill send 3 if you send 3 back?      0              17.0\n",
       "1                            perhaps      7             112.0\n",
       "2                        perhaps not      7             118.0\n",
       "3   Sending 4 back to you this round      0              17.0\n",
       "4  u and me dawg lets win this thing      0               9.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/labeled_data.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_text_to_label = {}\n",
    "\n",
    "for vals in classes.values():\n",
    "    for sub_message_key, sub_message_val in vals['subclasses'].items():\n",
    "        class_text_to_label[sub_message_key] = sub_message_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48563218390804597\n"
     ]
    }
   ],
   "source": [
    "# TOP 5\n",
    "n_correct = 0\n",
    "errors = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    text, label = df.loc[df.index[i], ['text', 'submessage_label']]\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    predicted_labels = [class_text_to_label[pred] for pred in predicted_classes]\n",
    "    n_correct += 1 if label in predicted_labels else 0\n",
    "    if label not in predicted_labels:\n",
    "        errors.append((text, predicted_classes))\n",
    "\n",
    "print(n_correct / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text message: they will prolly do one more round of attack on moz and then move to asiago\n",
      "Predicted conversions: ['Who should I attack?' 'Attack {player_names}' 'Next round'\n",
      " '{player_names} are plotting an attack against {player_names}'\n",
      " '{player_names} need to attack {player_names}']\n",
      "\n",
      "Original text message: i got 7 for you, we chillin\n",
      "Predicted conversions: ['Sounds good' 'Thanks' 'I messed up' \"I haven't been receiving\"\n",
      " 'Good game']\n",
      "\n",
      "Original text message: ok so\n",
      "Predicted conversions: ['Yes' 'All good' 'Me too' 'No' 'True']\n",
      "\n",
      "Original text message: Yea I see. Lets all take 6 from abinadi. Maybe hold the rest for protection\n",
      "Predicted conversions: ['Leave the group?' 'All good' \"Don't attack {player_names}\"\n",
      " 'Drop {player_names} from the group?' \"Let's form a secret group\"]\n",
      "\n",
      "Original text message: okay\n",
      "Predicted conversions: ['Yes' 'All good' 'No' 'True' 'No problem']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(errors)\n",
    "for text, preds in errors[:5]:\n",
    "    print(f'Original text message: {text}')\n",
    "    print(f'Predicted conversions: {preds}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5531609195402298\n"
     ]
    }
   ],
   "source": [
    "# TOP 7\n",
    "n_correct = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    text, label = df.loc[df.index[i], ['text', 'submessage_label']]\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings, top_n=7)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    predicted_labels = [class_text_to_label[pred] for pred in predicted_classes]\n",
    "    n_correct += 1 if label in predicted_labels else 0\n",
    "\n",
    "print(n_correct / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.639367816091954\n"
     ]
    }
   ],
   "source": [
    "# TOP 10\n",
    "n_correct = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    text, label = df.loc[df.index[i], ['text', 'submessage_label']]\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings, top_n=10)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    predicted_labels = [class_text_to_label[pred] for pred in predicted_classes]\n",
    "    n_correct += 1 if label in predicted_labels else 0\n",
    "\n",
    "print(n_correct / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3850574712643678\n"
     ]
    }
   ],
   "source": [
    "# TOP 3\n",
    "n_correct = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    text, label = df.loc[df.index[i], ['text', 'submessage_label']]\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings, top_n=3)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    predicted_labels = [class_text_to_label[pred] for pred in predicted_classes]\n",
    "    n_correct += 1 if label in predicted_labels else 0\n",
    "\n",
    "print(n_correct / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24856321839080459\n"
     ]
    }
   ],
   "source": [
    "# TOP 1\n",
    "n_correct = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    text, label = df.loc[df.index[i], ['text', 'submessage_label']]\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings, top_n=1)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    predicted_labels = [class_text_to_label[pred] for pred in predicted_classes]\n",
    "    n_correct += 1 if label in predicted_labels else 0\n",
    "\n",
    "print(n_correct / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "gen_model_checkpoint = 'distilbert/distilgpt2'\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_checkpoint)\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "gen_model = TFAutoModelForCausalLM.from_pretrained(gen_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    model_inputs = gen_tokenizer(text, padding=True, truncation=True, max_length=128, add_special_tokens=False, return_tensors='tf')\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ill send 3 if you send 3 back?\n",
      "['{player_names} how much are you sending?']\n"
     ]
    }
   ],
   "source": [
    "text = df.loc[df.index[0], 'text']\n",
    "text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "indices = convert(text_embedding, classes_embeddings, top_n=1)\n",
    "predicted_classes = classes_text[indices]\n",
    "\n",
    "print(text)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a short, human-like message (no more than one or two sentences), that is similar in meaning to the following text and is in the context of a game where players exchange popularity tokens: {player_names} how much are you sending?'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f'Generate a short, human-like message (no more than one or two sentences), that is similar in meaning to the following text and is in the context of a game where players exchange popularity tokens: {predicted_classes[0]}'\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {player_username} how many times a player has already received a token? {player_name} or {username_password_password} how\n"
     ]
    }
   ],
   "source": [
    "tokenized_prompt = tokenize(prompt)\n",
    "output = gen_model.generate(**tokenized_prompt, max_length=tokenized_prompt['input_ids'].shape[-1] + 30, do_sample=True, top_k=50)\n",
    "decoded_output = gen_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct, n_samples = 0, 100\n",
    "\n",
    "for i in range(100):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'{i + 1}%')\n",
    "    text = df.loc[df.index[i], 'text']\n",
    "    text_embedding = model.encode(text, convert_to_numpy=True)\n",
    "    indices = convert(text_embedding, classes_embeddings, top_n=1)\n",
    "    predicted_classes = classes_text[indices]\n",
    "    best_class = predicted_classes[0]\n",
    "    prompt = f'Generate a short, human-like message (no more than one or two sentences), that is similar in meaning to the following text and is in the context of a game where players exchange popularity tokens: {best_class}'\n",
    "    tokenized_prompt = tokenize(prompt)\n",
    "    output = gen_model.generate(**tokenized_prompt, max_length=tokenized_prompt['input_ids'].shape[-1] + 30, do_sample=True, top_k=5)\n",
    "    decoded_output = gen_tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):]\n",
    "    gen_embedding = model.encode(decoded_output, convert_to_numpy=True)\n",
    "    indices = convert(gen_embedding, classes_embeddings, top_n=5)\n",
    "    predicted_classes_new = classes_text[indices]\n",
    "    n_correct += 1 if best_class in predicted_classes_new else 0\n",
    "\n",
    "n_correct / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "501r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
